behaviors:
    Heli:
        trainer_type: ppo
        hyperparameters:
            batch_size: 256         #   Discrete act typical range of 32-512 (prev id59: 2048)
            buffer_size: 10240
            learning_rate: 0.0002   #   0.0003  
            beta: 0.001              #   def: 0.01 ENTROPY higher makes the policy "more random." & explore more :: TR 0.0001 - 0.01
            epsilon: 0.2            #   (epsilon) small probability  it instead takes a random action
            lambd: 0.95
            num_epoch: 3
            learning_rate_schedule: linear
        network_settings:
            normalize: true         #   observations
            hidden_units: 512
            num_layers: 2           #   def 2
            use_recurrent: true     #   mem
            sequence_length: 256    #   mem 512
            memory_size: 256        #   mem 128
            vis_encode_type: simple
        reward_signals:
            extrinsic:
                gamma: 0.99         #   The discount factor. 0 (priorite immediate R) or closer to 1 (valuing future rewards more).
                strength: 1.0
            curiosity:
                gamma: 0.99
                strength: 0.02      #   def: 0.02 [MAX Explore: 0.1] ratio to Extrinsic strength
                network_settings:
                    hidden_units: 256
                    encoding_size: 128  #   def: 64
                learning_rate: 0.0003   #   update the intrinsic curiosity module
        max_steps: 5000000
        time_horizon: 512           #   256 experience will be divided into segments of t_h.    Typical Range: 32 - 2048
        summary_freq: 5000
        keep_checkpoints: 15
        checkpoint_interval: 250000
        threaded: true

    LoaderG:
        trainer_type: ppo
        hyperparameters:
            batch_size: 128
            buffer_size: 2048
            learning_rate: 0.0003
            beta: 0.01
            epsilon: 0.2
            lambd: 0.95
            num_epoch: 3
            learning_rate_schedule: linear
        network_settings:
            normalize: false
            hidden_units: 512
            num_layers: 2
            vis_encode_type: simple
        reward_signals:
            extrinsic:
                gamma: 0.99
                strength: 1.0
            curiosity:
                gamma: 0.99
                strength: 0.02
                network_settings:
                    hidden_units: 256
                learning_rate: 0.0003
        max_steps: 2000000
        time_horizon: 512
        summary_freq: 5000
        threaded: true
